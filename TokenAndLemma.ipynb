{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "import io\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "sw_nltk = stopwords.words('english')\n",
    "fTokens = open(\"fileTokens.txt\", mode = \"a\", encoding=\"utf-8\")\n",
    "for i in range(100):\n",
    "    filePath = r\"C:\\Users\\USER\\{}.txt\".format(i+1)\n",
    "    f = io.open(filePath, \"r\", encoding=\"utf-8\")\n",
    "    sentence = f.read()\n",
    "    line = re.sub(r\"[][\\d]\", \"\", sentence)\n",
    "    line = line.replace(\"—\", \" \")\n",
    "    line = line.translate(str.maketrans('', '', string.punctuation))\n",
    "    word_list_cleaned = [word for word in line.split() if word.lower() not in sw_nltk]\n",
    "    new_text = \" \".join(word_list_cleaned)\n",
    "    word_list = nltk.word_tokenize(new_text)\n",
    "\n",
    "    for word in word_list:\n",
    "        fTokens.write(word)\n",
    "        fTokens.write(\"\\n\")\n",
    "      \n",
    "    f.close()\n",
    "fTokens.close()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#я не совсем уверена, что правильно поняла задание, если честно\n",
    "#вот у нас есть токены, я их лемматизирую - мне надо только одну лемму найти или все леммы?\n",
    "import nltk\n",
    "import io\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "fTokens = open(\"fileTokens.txt\", mode = \"r\", encoding = \"utf-8\")\n",
    "fLemmas = open(\"fileLemmasFromTokens.txt\", mode = \"a\", encoding = \"utf-8\")\n",
    "words = fTokens.read()\n",
    "contents_split = words.splitlines()\n",
    "for word in contents_split:\n",
    "    fLemmas.write(word + \" \" + lemmatizer.lemmatize(word) + \"\\n\")\n",
    "fLemmas.close()\n",
    "fTokens.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(list1, list2):\n",
    "    return list(set(list1).intersection(list2))\n",
    "def union(list1, list2):\n",
    "    return list(set(list1).union(list2))\n",
    "def notin(list1, list2):\n",
    "    return [filter(lambda x: x not in list1, sublist) for sublist in list2]\n",
    "\n",
    "dict = {}\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "import io\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "sw_nltk = stopwords.words('english')\n",
    "fTokens = open(\"fileTokens.txt\", mode = \"a\", encoding=\"utf-8\")\n",
    "for i in range(100):\n",
    "    filePath = r\"C:\\Users\\USER\\{}.txt\".format(i+1)\n",
    "    f = io.open(filePath, \"r\", encoding=\"utf-8\")\n",
    "    sentence = f.read()\n",
    "    line = re.sub(r\"[][\\d]\", \"\", sentence)\n",
    "    line = line.replace(\"—\", \" \")\n",
    "    line = line.translate(str.maketrans('', '', string.punctuation))\n",
    "    word_list_cleaned = [word for word in line.split() if word.lower() not in sw_nltk]\n",
    "    new_text = \" \".join(word_list_cleaned)\n",
    "    word_list = nltk.word_tokenize(new_text)\n",
    "    converted_list = [x.lower() for x in word_list]\n",
    "    uniquewordlist = set(converted_list)\n",
    "    for id, val in enumerate(uniquewordlist):\n",
    "        if val not in dict:\n",
    "            dict[val] = []\n",
    "        if val in dict:\n",
    "            dict[val].append(i)        \n",
    "    f.close()\n",
    "fTokens.close()\n",
    "\n",
    "\n",
    "#boolean search, created sets\n",
    "\n",
    "fInvertedIndex = open(\"invertedIndex.txt\", mode = \"a\", encoding=\"utf-8\")\n",
    "for i in dict:\n",
    "    fInvertedIndex.write(i + \" \" + str(dict[i]) + \"\\n\")\n",
    "fInvertedIndex.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 задание\n",
    "import io\n",
    "import math\n",
    "\n",
    "fileToken = io.open(\"fileTokens.txt\", mode = \"r\", encoding = \"UTF-8\")\n",
    "list1 = fileToken.readlines()\n",
    "mlist = [line.rstrip('\\n') for line in list1]\n",
    "tf = {}\n",
    "\n",
    "myset = set(mlist)\n",
    "\n",
    "mylist = []\n",
    "\n",
    "for word in myset:\n",
    "    mylist.append(word.lower())\n",
    "\n",
    "for i in range(100):\n",
    "    filePath = r\"C:\\Users\\USER\\{}.txt\".format(i+1)\n",
    "    f = io.open(filePath, \"r\", encoding=\"utf-8\")\n",
    "    file = f.read()\n",
    "    word_list_file = [word.lower() for word in file.split()]\n",
    "    lengthwordlist = len(word_list_file)\n",
    "    for j in mylist:\n",
    "        if j not in tf:\n",
    "            tf[j] = []\n",
    "        if j in tf:\n",
    "            tf[j].append(word_list_file.count(j)/lengthwordlist)\n",
    "\n",
    "idf = {}\n",
    "\n",
    "for j in mylist:\n",
    "    if j not in idf:\n",
    "        idf[j] = []\n",
    "        if (sum(i>0.0 for i in tf[j]) != 0):\n",
    "            idf[j].append(math.log10(100/sum(i>0.0 for i in tf[j])))\n",
    "\n",
    "import operator\n",
    "tf_idf = {}\n",
    "\n",
    "def multiplytwolists(list1, list2):\n",
    "    tf_idf_list = []\n",
    "    for i in list1:\n",
    "        if len(list2) > 0:\n",
    "            tf_idf_list.append(i*list2[0])\n",
    "        else:\n",
    "            tf_idf_list.append(0)\n",
    "    return tf_idf_list\n",
    "    \n",
    "for j in mylist:\n",
    "    if j not in tf_idf:\n",
    "        tf_idf[j] = []\n",
    "        tf_idf_list = multiplytwolists(tf[j], idf[j])\n",
    "        tf_idf[j].append(tf_idf_list)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileTf = io.open(\"fileTF.txt\", mode = \"a\", encoding=\"UTF-8\")\n",
    "for word in mylist:\n",
    "    fileTf.write(word + \" \" + str(idf[word]) + \" \" + str(tf_idf[word]) + \"\\n\")\n",
    "fileTf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "введите что-нибудь bran\n",
      "                                                Сайты\n",
      "0    https://awoiaf.westeros.org/index.php/Kingsguard\n",
      "1    https://awoiaf.westeros.org/index.php/Robb_Stark\n",
      "2    https://awoiaf.westeros.org/index.php/Kingsguard\n",
      "3   https://awoiaf.westeros.org/index.php/Cersei_L...\n",
      "4   https://awoiaf.westeros.org/index.php/Duncan_t...\n",
      "5   https://awoiaf.westeros.org/index.php/The_Wind...\n",
      "6     https://awoiaf.westeros.org/index.php/Wolfswood\n",
      "7   https://awoiaf.westeros.org/index.php/Duncan_t...\n",
      "8   https://awoiaf.westeros.org/index.php/Craster%...\n",
      "9   https://awoiaf.westeros.org/index.php/Tommen_B...\n",
      "10  https://awoiaf.westeros.org/index.php/A_Dance_...\n",
      "11  https://awoiaf.westeros.org/index.php/Stannis_...\n",
      "12   https://awoiaf.westeros.org/index.php/Winterfell\n",
      "13  https://awoiaf.westeros.org/index.php/A_Dance_...\n",
      "14  https://awoiaf.westeros.org/index.php/A_Dance_...\n",
      "15  https://awoiaf.westeros.org/index.php/House_Bo...\n",
      "16  https://awoiaf.westeros.org/index.php/A_Clash_...\n",
      "17     https://awoiaf.westeros.org/index.php/The_Wall\n",
      "18  https://awoiaf.westeros.org/index.php/A_Game_o...\n",
      "19  https://awoiaf.westeros.org/index.php/Howland_...\n",
      "20  https://awoiaf.westeros.org/index.php/Tommen_B...\n",
      "21  https://awoiaf.westeros.org/index.php/A_Game_o...\n",
      "22   https://awoiaf.westeros.org/index.php/Bran_Stark\n",
      "23  https://awoiaf.westeros.org/index.php/Boros_Bl...\n",
      "24        https://awoiaf.westeros.org/index.php/North\n",
      "25   https://awoiaf.westeros.org/index.php/Long_Night\n",
      "26         https://awoiaf.westeros.org/index.php/Wall\n",
      "27  https://awoiaf.westeros.org/index.php/A_Song_o...\n",
      "28  https://awoiaf.westeros.org/index.php/Prince_o...\n",
      "29  https://awoiaf.westeros.org/index.php/Brynden_...\n",
      "30  https://awoiaf.westeros.org/index.php/Benjen_S...\n",
      "31  https://awoiaf.westeros.org/index.php/Mutiny_a...\n",
      "32  https://awoiaf.westeros.org/index.php/A_Dance_...\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "inverted_index = parse_inverted_index(\"invertedIndex\")\n",
    "sites = parse_sites(\"index\")\n",
    "\n",
    "query = input(\"введите что-нибудь \")\n",
    "result = parseString(query, inverted_index)\n",
    "urls_list = get_urls(result, sites)\n",
    "urls_pd = pd.DataFrame(urls_list, columns=[\"Сайты\"])\n",
    "print(urls_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pymorphy2\n",
    "def parse_inverted_index(filename):\n",
    "    i_index_ = {}\n",
    "    with open(str(filename) + \".txt\", \"r\", encoding=\"UTF-8\") as f:\n",
    "        for line in f:  \n",
    "            word = line[:line.find(' ')]\n",
    "            array = line[line.find(' '):]\n",
    "            array = array[2:-2].split(',')\n",
    "            array = [s.strip() for s in array]\n",
    "            i_index_[word] = set(array)\n",
    "    return i_index_\n",
    "\n",
    "def parse_sites(filename):\n",
    "    sites_ = {}\n",
    "    with open(str(filename) + \".txt\", \"r\", encoding = \"UTF-8\") as f1:\n",
    "        for line in f1:\n",
    "            sites_array = line.strip(\"\\n\").split(' ')\n",
    "            sites_[sites_array[0]] = sites_array[1]\n",
    "    return sites_\n",
    "\n",
    "def get_urls(_set, _sites):\n",
    "    urls = list()\n",
    "    for item in _set:\n",
    "        item = str(int(item)+1)\n",
    "        urls.append(_sites[item])\n",
    "       \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    " def parseString(query, inverted_index):\n",
    "        result_set = set()\n",
    "        operation = None\n",
    "        analyzer = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "        first = True\n",
    "        for word in re.split(\" +(AND|OR) +\", query):\n",
    "            # word will be in ['foo', 'OR', 'bar', 'AND', 'NOT gazonk']\n",
    "\n",
    "            inverted = False  # for \"NOT word\" operations\n",
    "\n",
    "            if word in ['AND', 'OR']:\n",
    "                operation = word\n",
    "                continue\n",
    "\n",
    "            if word.find('NOT ') == 0:\n",
    "                if operation == 'OR':\n",
    "                    # generally \"OR NOT\" operation does not make sense, but if it does in your case, you\n",
    "                    # should update this if() accordingly\n",
    "                    continue\n",
    "\n",
    "                inverted = True\n",
    "                # the word is inverted!\n",
    "                realword = word[4:]\n",
    "            else:\n",
    "                realword = word\n",
    "\n",
    "            if first:\n",
    "                normal_form = analyzer.parse(realword)[0].normal_form\n",
    "                if normal_form not in inverted_index:\n",
    "                    print(\"There is no '{}' in inverted index\".format(realword))\n",
    "                    return set()\n",
    "                result_set = inverted_index[normal_form]\n",
    "                first = False\n",
    "\n",
    "            if operation is not None:\n",
    "                # now we need to match the key and the filenames it contains:\n",
    "                normal_form = analyzer.parse(realword)[0].normal_form\n",
    "                if normal_form not in inverted_index:\n",
    "                    print(\"There is no '{}' in inverted index\".format(realword))\n",
    "                    return set()\n",
    "                current_set = inverted_index[normal_form]\n",
    "\n",
    "                if operation == 'AND':\n",
    "                    if inverted is True:\n",
    "                        result_set -= current_set\n",
    "                    else:\n",
    "                        result_set &= current_set\n",
    "                elif operation == 'OR':\n",
    "                    result_set |= current_set\n",
    "            operation = None\n",
    "        return result_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
